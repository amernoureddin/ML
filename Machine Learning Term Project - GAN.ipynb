{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EECS 461 Machine Learning Term Project - İstanbul Şehir Üniversitesi\n",
    "#### Professor: Mehmet Serkan Apaydın\n",
    "#### Student: Amer Nour Eddin\n",
    "\n",
    "#### 08.Feb.2018\n",
    "--------------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# << Generative Adversarial Networks GANs >>\n",
    "=================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Preface <<< "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between a Generative and a discriminative model?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/1.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generative Models:\n",
    "Aim to model how the data is generated. From [ P(x|c)×P(c)P(x|c)×P(c) ] we can obtain [ P(c|x)P(c|x) ] (Bayes' Theorem). - They (Generative models) try to learn a joint probability distribution P(x,c)P(x,c)\n",
    "##### Pros:\n",
    "We have the knowledge about the data distribution.\t\n",
    "##### Cons:\n",
    "Very expensive to get (a lot of parameters)\n",
    "Need lots of data\n",
    "\n",
    "#### Discriminative Models:  (The kind of Models we studied in class)\n",
    "Aim at learning P(c|x)P(c|x) by using probabilistic approaches (e.g., logistic regression), or by mapping classes from a set of points (e.g., perceptrons and SVMs).\n",
    "##### Pros:\n",
    "Easy to model\n",
    "##### Cons:\n",
    "To classify, but not to generate the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have input data x and you want to classify the data into labels y. A generative model learns the joint probability distribution p(x, y) and a discriminative model learns the conditional probability distribution p(y|x) - which you should read as \"the probability of y given x\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a REALLY SIMPLE example. Suppose you have the following data in the form (x, y):\n",
    "\n",
    "### {(1, 0), (1, 0), (2, 0), (2, 1)}\n",
    "\n",
    "##### p(x, y) is:\n",
    "|     | y=0 | y=1 |\n",
    "|:---:|:---:|:---:|\n",
    "| x=1 | 1/2 |  0  |\n",
    "| x=2 | 1/4 | 1/4 |\n",
    "                                                            Generative\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "##### p(y|x) is:                                                               \n",
    "|     | y=0 | y=1 | \n",
    "|:---:|:---:|:---:|\n",
    "| x=1 |  1  |  0  | \n",
    "| x=2 | 1/2 | 1/2 |\n",
    "                                                          Discriminative\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "If you take a few minutes to stare at those two matrices, you will understand the difference between the two probability distributions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](img/2.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So __discriminative algorithms__ learn the boundaries between classes while __generative algorithms__learn the distribution of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Generative Adversarial Network? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Adversarial Network GAN is a relatively new Machine Learning topic introduced by by Ian Goodfellow and his colleagues at the University of Montreal in 2014. So obviously it is a new field and it is now from the hottest research topics in Machine Learning.\n",
    "\n",
    "GAN, is a type of __generative model__ – a model that looks at the training data drawn from a certain distribution and tries to estimate that distribution. New samples obtained from such model look like original training samples.\n",
    "\n",
    "\n",
    "In words, GANs basically contains tow neural nets one is called the discriminator D and the other is called the generator G. D is playing the role of a detector (classifier) that will be trained on a real data and can after training decide on a given sample that it is real or fake, in the same time G is playing the role of the forger for D (adversary) so from a given noise or latent space it tries to generat a sample that will hopfully fool D and make it say that it is a real sample, and G keeps improving his samples as he gets caught by D until we reach a point that D cannot decide whether the sample is real or fake at that point we discard D and we now have a fully skilled generative network G that can generate nearly real samples from noise!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](img/3.JPG)\n",
    "![title](img/4.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator\n",
    "+\tDraw some parameter z from a source of randomness, e.g. a normal distribution\n",
    "+\tApply a function f such that we get x′=G(u,w)\n",
    "+\tCompute the gradient with respect to w to minimize logp(y=fake|x′) === (cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator\n",
    "+\tImprove the accuracy of a binary classifier f, i.e. maximize logp(y=fake|x′) for fake data and logp(y=true|x) for real data.\n",
    "+\tThere are two optimization problems running simultaneously, and the optimization terminates if a stalemate has been reached.\n",
    "+\tThe models play two distinct (literally, adversarial) roles.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](img/5.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, practicaly, there are really only 5 components to think about:\n",
    "   *\tR: The original, genuine data set\n",
    "   *\tI: The random noise that goes into the generator as a source of entropy\n",
    "   *\tG: The generator which tries to copy/mimic the original data set\n",
    "   *\tD: The discriminator which tries to tell apart G’s output from R \n",
    "   \n",
    " __* The actual ‘training’ loop is where we teach G to trick D and D to beware G.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use cases of GANs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](img/6.JPG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](img/8.JPG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, Generate images, videos, text, any data type!\n",
    "\n",
    "Researchers from Insilico Medicine proposed an approach of artificially intelligent drug discovery using GANs. The goal is to train the Generator to sample drug candidates for a given disease as precisely as possible to existing drugs from a Drug Database.\n",
    "\n",
    "\n",
    "![title](img/7.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many types of GANs;\n",
    "\n",
    "+ DCGAN(Deep Convolutional GAN)\n",
    "+ CGAN (Conditional GAN)\n",
    "+ Improved DCGAN\n",
    "+ etc. \n",
    "\n",
    "I won't explain these types since my focus will be on one specific type which is called __WGAN (Wasserstein Generative Adverserial Network)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs (vanilla GANs) have always had problems with convergence and, as a consequence, you don’t really know when to stop training them. In other words, the loss function doesn’t correlate with image quality. This is a big problem because:\n",
    "\n",
    "*\tYou need to be constantly looking at the samples to tell whether your model is training correctly or not.\n",
    "*\tYou don’t know when to stop training (no perfect convergence).\n",
    "*\tYou don’t have a numerical value that tells you how well you are tuning the parameters.\n",
    "\n",
    "This interpretability issue is one of the problems that Wasserstein GANs aims to solve. How? GANs (Vanilla GANs) can be interpreted to minimize the __Jensen-Shannon divergence__, which is 0 if the real and fake distribution don’t overlap (which is usually the case). So, instead of minimizing the JS divergence, the authors use the __Wasserstein distance aka. Eearth Mover distance__, which describes the distance between the “points” from one distribution to the other.\n",
    "\n",
    "So, WassGAN has a loss function that correlates with image quality and ***enables convergence***. It is also ***more stable***, meaning that it is not as dependent on the architecture.\n",
    "\n",
    "So, if you are __looking for a state-of-the-art GAN with the highest training stability__ and __want an informative and interpretable loss function.__ your choice is to use the WGAN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below is an example plot for the (Loss) of a normal GAN that can clearly show you the instability problem when training the GANs\n",
    "\n",
    "P.S. I will create my own plot for my own WGAN at the end of my code which will show the difference and how WGAN is more stable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](img/9.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow implementation for the WGAN \n",
    "=============================================================\n",
    "###### + detailed comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a very simple and straight forward implementation for a WGAN using Tensorflow framework.\n",
    "\n",
    "**The MNIST dataset is used as the real dataset and the code is a simple Wasserstein Generative Adverserial Network architecture that tries learning how to produce the MNIST dataset from a random noise..** (So the desired output is the MNIST digits, from input of random noise)\n",
    "\n",
    "*You can follow my comments line by line in case there are any ambiguous parts that you didn't understrand.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining some constants\n",
    "\n",
    "mb_size = 32 # mini batch size\n",
    "X_dim = 784 # MNIST data input (img shape: 28*28)\n",
    "z_dim = 10 # MNIST total classes (0-9 digits)\n",
    "h_dim = 128 # hidden layer number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ../../MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "\n",
    "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet at 0x23d480adf60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Xavier initilization function for implementing the random weight initialization for the neural network\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>>>>>> Construction Phase <<<<<<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, X_dim]) # a placeholder for the input of the Discriminator (real data)\n",
    "\n",
    "# Discriminator First Layar\n",
    "D_W1 = tf.Variable(xavier_init([X_dim, h_dim])) # wights\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[h_dim])) # bias\n",
    "\n",
    "# Discriminator Second Layar\n",
    "D_W2 = tf.Variable(xavier_init([h_dim, 1])) #wights\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1])) # bias\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]  # theta_D is a list of the Discriminator parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = tf.placeholder(tf.float32, shape=[None, z_dim]) # a placeholder for the input of the Generator (random noise)\n",
    "\n",
    "# Generator Second Layar\n",
    "G_W1 = tf.Variable(xavier_init([z_dim, h_dim])) # wights\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[h_dim])) # bias\n",
    "\n",
    "# Generator Second Layar\n",
    "G_W2 = tf.Variable(xavier_init([h_dim, X_dim])) # wights\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[X_dim])) # bias\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2] # theta_D is a list of the Generator parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random values size m*n\n",
    "def sample_z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator Neural Network function\n",
    "def generator(z):\n",
    "    # multiply input (z) matrix with (wights) matrix and add the result to bias values and feed all of that into a RELU acti. fn.\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1) # RELU Activiation function used\n",
    "    # same for the second layer\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2 \n",
    "    # sigmoid funcion for the output\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob) \n",
    "    return G_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discriminator Neural Network function\n",
    "def discriminator(x):\n",
    "    # multiply input (X) matrix with (wights) matrix and add the result to bias values and feed all of that into a RELU acti. fn.\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1) # RELU Activiation function used\n",
    "    out = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the following (plot function) is for extracting the outputed images from the generator\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G_sample = generator(z) # instantiating a generator function (Generator NN)\n",
    "\n",
    "D_real = discriminator(X) # Classify real data! (Discriminator NN)\n",
    "D_fake = discriminator(G_sample) # classify (generator G_sample) generated data! (Discriminator NN)\n",
    "\n",
    "# Discriminator loss (cost function)\n",
    "D_loss = tf.reduce_mean(D_real) - tf.reduce_mean(D_fake) \n",
    "# Generator loss (cost function)\n",
    "G_loss = -tf.reduce_mean(D_fake) \n",
    "\n",
    "# Solving or optimizing the cost function of the discriminator \n",
    "D_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4).minimize(-D_loss, var_list=theta_D))\n",
    "# Solving or optimizing the cost function of the generator \n",
    "G_solver = (tf.train.RMSPropOptimizer(learning_rate=1e-4).minimize(G_loss, var_list=theta_G))\n",
    "\n",
    "clip_D = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in theta_D] # clip the Discriminator's theta values [-0.01 - 0.01]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>>>>>> Execution Phase <<<<<<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 ---> Discriminator loss: -0.0001722 | Generator loss: -0.004419\n",
      "Iteration: 100 ---> Discriminator loss: 1.866 | Generator loss: 1.388\n",
      "Iteration: 200 ---> Discriminator loss: 1.755 | Generator loss: 1.25\n",
      "Iteration: 300 ---> Discriminator loss: 1.563 | Generator loss: 0.9834\n",
      "Iteration: 400 ---> Discriminator loss: 1.311 | Generator loss: 0.8105\n",
      "Iteration: 500 ---> Discriminator loss: 1.082 | Generator loss: 0.7432\n",
      "Iteration: 600 ---> Discriminator loss: 0.6516 | Generator loss: 0.577\n",
      "Iteration: 700 ---> Discriminator loss: 0.444 | Generator loss: 0.567\n",
      "Iteration: 800 ---> Discriminator loss: 0.291 | Generator loss: 0.495\n",
      "Iteration: 900 ---> Discriminator loss: 0.1747 | Generator loss: 0.3538\n",
      "Iteration: 1000 ---> Discriminator loss: 0.1087 | Generator loss: 0.1371\n",
      "Iteration: 1100 ---> Discriminator loss: 0.07424 | Generator loss: 0.05151\n",
      "Iteration: 1200 ---> Discriminator loss: 0.02353 | Generator loss: 0.1423\n",
      "Iteration: 1300 ---> Discriminator loss: 0.03588 | Generator loss: -0.006396\n",
      "Iteration: 1400 ---> Discriminator loss: 0.01009 | Generator loss: -0.0221\n",
      "Iteration: 1500 ---> Discriminator loss: -0.006728 | Generator loss: -0.01399\n",
      "Iteration: 1600 ---> Discriminator loss: -0.00364 | Generator loss: -0.03834\n",
      "Iteration: 1700 ---> Discriminator loss: 0.004079 | Generator loss: -0.01583\n",
      "Iteration: 1800 ---> Discriminator loss: -0.001397 | Generator loss: 0.03745\n",
      "Iteration: 1900 ---> Discriminator loss: 0.01467 | Generator loss: -0.08773\n",
      "Iteration: 2000 ---> Discriminator loss: 0.0015 | Generator loss: 0.05813\n",
      "Iteration: 2100 ---> Discriminator loss: 0.003125 | Generator loss: -0.02784\n",
      "Iteration: 2200 ---> Discriminator loss: -0.003974 | Generator loss: -0.05084\n",
      "Iteration: 2300 ---> Discriminator loss: -0.002742 | Generator loss: -0.01168\n",
      "Iteration: 2400 ---> Discriminator loss: -0.002219 | Generator loss: -0.05382\n",
      "Iteration: 2500 ---> Discriminator loss: -9.033e-05 | Generator loss: 0.02522\n",
      "Iteration: 2600 ---> Discriminator loss: -0.003919 | Generator loss: 0.005442\n",
      "Iteration: 2700 ---> Discriminator loss: 0.002388 | Generator loss: 0.02106\n",
      "Iteration: 2800 ---> Discriminator loss: 0.005006 | Generator loss: -0.05325\n",
      "Iteration: 2900 ---> Discriminator loss: 0.003125 | Generator loss: 0.02574\n",
      "Iteration: 3000 ---> Discriminator loss: 0.002744 | Generator loss: -0.01463\n",
      "Iteration: 3100 ---> Discriminator loss: 0.00192 | Generator loss: -0.01618\n",
      "Iteration: 3200 ---> Discriminator loss: -0.0003237 | Generator loss: 0.09752\n",
      "Iteration: 3300 ---> Discriminator loss: -0.002131 | Generator loss: 0.04974\n",
      "Iteration: 3400 ---> Discriminator loss: 0.001867 | Generator loss: 0.05338\n",
      "Iteration: 3500 ---> Discriminator loss: 0.01152 | Generator loss: -0.1186\n",
      "Iteration: 3600 ---> Discriminator loss: 1.583e-05 | Generator loss: 0.04117\n",
      "Iteration: 3700 ---> Discriminator loss: 0.01144 | Generator loss: 0.07178\n",
      "Iteration: 3800 ---> Discriminator loss: 0.004655 | Generator loss: 0.02911\n",
      "Iteration: 3900 ---> Discriminator loss: 0.01376 | Generator loss: 0.03473\n",
      "Iteration: 4000 ---> Discriminator loss: 0.03532 | Generator loss: 0.04483\n",
      "Iteration: 4100 ---> Discriminator loss: 0.06896 | Generator loss: 0.006327\n",
      "Iteration: 4200 ---> Discriminator loss: 0.09151 | Generator loss: -0.004228\n",
      "Iteration: 4300 ---> Discriminator loss: 0.06745 | Generator loss: -0.03444\n",
      "Iteration: 4400 ---> Discriminator loss: 0.079 | Generator loss: -0.01986\n",
      "Iteration: 4500 ---> Discriminator loss: 0.06788 | Generator loss: -0.02685\n",
      "Iteration: 4600 ---> Discriminator loss: 0.0549 | Generator loss: 0.004225\n",
      "Iteration: 4700 ---> Discriminator loss: 0.07113 | Generator loss: 0.003449\n",
      "Iteration: 4800 ---> Discriminator loss: 0.06269 | Generator loss: -0.007239\n",
      "Iteration: 4900 ---> Discriminator loss: 0.06401 | Generator loss: 0.0007231\n",
      "Iteration: 5000 ---> Discriminator loss: 0.0592 | Generator loss: -0.02323\n",
      "Iteration: 5100 ---> Discriminator loss: 0.04501 | Generator loss: -0.01479\n",
      "Iteration: 5200 ---> Discriminator loss: 0.05571 | Generator loss: 0.003463\n",
      "Iteration: 5300 ---> Discriminator loss: 0.05349 | Generator loss: -0.01853\n",
      "Iteration: 5400 ---> Discriminator loss: 0.06154 | Generator loss: -0.01887\n",
      "Iteration: 5500 ---> Discriminator loss: 0.04584 | Generator loss: -0.004327\n",
      "Iteration: 5600 ---> Discriminator loss: 0.05585 | Generator loss: -0.02196\n",
      "Iteration: 5700 ---> Discriminator loss: 0.05074 | Generator loss: -0.0116\n",
      "Iteration: 5800 ---> Discriminator loss: 0.04194 | Generator loss: 0.01627\n",
      "Iteration: 5900 ---> Discriminator loss: 0.04069 | Generator loss: 0.001608\n",
      "Iteration: 6000 ---> Discriminator loss: 0.03648 | Generator loss: -0.02203\n",
      "Iteration: 6100 ---> Discriminator loss: 0.04014 | Generator loss: -0.006799\n",
      "Iteration: 6200 ---> Discriminator loss: 0.04768 | Generator loss: -0.01778\n",
      "Iteration: 6300 ---> Discriminator loss: 0.03643 | Generator loss: -0.008161\n",
      "Iteration: 6400 ---> Discriminator loss: 0.04454 | Generator loss: -0.001805\n",
      "Iteration: 6500 ---> Discriminator loss: 0.04442 | Generator loss: -0.02623\n",
      "Iteration: 6600 ---> Discriminator loss: 0.03576 | Generator loss: -0.01259\n",
      "Iteration: 6700 ---> Discriminator loss: 0.04656 | Generator loss: -0.02031\n",
      "Iteration: 6800 ---> Discriminator loss: 0.0329 | Generator loss: -0.0267\n",
      "Iteration: 6900 ---> Discriminator loss: 0.03876 | Generator loss: -0.008109\n",
      "Iteration: 7000 ---> Discriminator loss: 0.04208 | Generator loss: 0.003815\n",
      "Iteration: 7100 ---> Discriminator loss: 0.04396 | Generator loss: -0.02208\n",
      "Iteration: 7200 ---> Discriminator loss: 0.03573 | Generator loss: -0.01786\n",
      "Iteration: 7300 ---> Discriminator loss: 0.03503 | Generator loss: -0.01262\n",
      "Iteration: 7400 ---> Discriminator loss: 0.0449 | Generator loss: -0.0263\n",
      "Iteration: 7500 ---> Discriminator loss: 0.03994 | Generator loss: -0.01685\n",
      "Iteration: 7600 ---> Discriminator loss: 0.04037 | Generator loss: -0.006621\n",
      "Iteration: 7700 ---> Discriminator loss: 0.04632 | Generator loss: -0.02391\n",
      "Iteration: 7800 ---> Discriminator loss: 0.03694 | Generator loss: -0.0181\n",
      "Iteration: 7900 ---> Discriminator loss: 0.03192 | Generator loss: 0.005092\n",
      "Iteration: 8000 ---> Discriminator loss: 0.03663 | Generator loss: -0.01162\n",
      "Iteration: 8100 ---> Discriminator loss: 0.04003 | Generator loss: -0.02369\n",
      "Iteration: 8200 ---> Discriminator loss: 0.04232 | Generator loss: -0.0002248\n",
      "Iteration: 8300 ---> Discriminator loss: 0.04586 | Generator loss: -0.02128\n",
      "Iteration: 8400 ---> Discriminator loss: 0.03653 | Generator loss: -0.02488\n",
      "Iteration: 8500 ---> Discriminator loss: 0.03332 | Generator loss: -0.01273\n",
      "Iteration: 8600 ---> Discriminator loss: 0.03752 | Generator loss: -0.02989\n",
      "Iteration: 8700 ---> Discriminator loss: 0.03422 | Generator loss: -0.0275\n",
      "Iteration: 8800 ---> Discriminator loss: 0.03507 | Generator loss: -0.02421\n",
      "Iteration: 8900 ---> Discriminator loss: 0.03668 | Generator loss: -0.02978\n",
      "Iteration: 9000 ---> Discriminator loss: 0.02909 | Generator loss: -0.01998\n",
      "Iteration: 9100 ---> Discriminator loss: 0.03823 | Generator loss: -0.01559\n",
      "Iteration: 9200 ---> Discriminator loss: 0.03197 | Generator loss: -0.0155\n",
      "Iteration: 9300 ---> Discriminator loss: 0.03144 | Generator loss: -0.02647\n",
      "Iteration: 9400 ---> Discriminator loss: 0.02968 | Generator loss: -0.01462\n",
      "Iteration: 9500 ---> Discriminator loss: 0.03789 | Generator loss: -0.01137\n",
      "Iteration: 9600 ---> Discriminator loss: 0.02983 | Generator loss: -0.02143\n",
      "Iteration: 9700 ---> Discriminator loss: 0.03422 | Generator loss: -0.01592\n",
      "Iteration: 9800 ---> Discriminator loss: 0.03134 | Generator loss: -0.002442\n",
      "Iteration: 9900 ---> Discriminator loss: 0.03412 | Generator loss: -0.02562\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABMUAAAJRCAYAAABFiQAOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEYoBAAAAMDj/HwE55Xyyd5Y6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2465992feb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execution Phase #\n",
    "\n",
    "sess = tf.Session() # create a tensorflow session\n",
    "sess.run(tf.global_variables_initializer()) # initialize all global variables \n",
    "\n",
    "# Create an output folder in the working directory for storing the result\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "\n",
    "\n",
    "# the following function is for dynamically plotting the loss of both (Discriminator and Generator) during the training\n",
    "def plt_dynamic(x, y, y2, ax, colors=['b']):\n",
    "    for color in colors:       \n",
    "        ax.plot(x, y, color, label='Generator Error (Loss)')\n",
    "        ax2.plot(x, y2, 'g', label='Discriminator Error (Loss)')    \n",
    "    fig.canvas.draw()\n",
    "\n",
    "# also these are the plotting parameters and settings\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax2 = ax.twinx()\n",
    "ax.set_xlabel('Iterations') ; ax.set_ylabel('G Loss') ; ax2.set_ylabel('D Loss')\n",
    "ax.set_xlim(0,10000) ; ax.set_ylim(-1,2.5); ax2.set_ylim(-1,2.5)\n",
    "ax.legend(['Generator Error (Loss)'])\n",
    "ax2.legend(['Discriminator Error (Loss)'])\n",
    "ax.grid(color='b', linestyle='-', linewidth=0.2)\n",
    "xs, ys, y2s = [], [], []\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# TRAINING #\n",
    "\n",
    "i = 0\n",
    "\n",
    "for it in range(10000): # typically 1m iteration for the training loop !but I chose 10.000 for compelling reasons\n",
    "    for _ in range(5):\n",
    "        X_mb, _ = mnist.train.next_batch(mb_size) # train using batches (size of batch: 32) defined previously\n",
    "            \n",
    "            # compute the Discriminator loss\n",
    "        _, D_loss_curr, _ = sess.run([D_solver, D_loss, clip_D], feed_dict={X: X_mb, z: sample_z(mb_size, z_dim)})\n",
    "        # compute the Generator loss\n",
    "    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={z: sample_z(mb_size, z_dim)})\n",
    "    \n",
    "    # recording the loss values for plotting it daynamically\n",
    "    xs.append(it)\n",
    "    ys.append(G_loss_curr)\n",
    "    y2s.append(D_loss_curr)\n",
    "    plt_dynamic(xs, ys, y2s, ax)\n",
    "    time.sleep(.2)\n",
    "\n",
    "    \n",
    "    # Every 100 Iteration show me the D & G Loss (Which is the unique property of WGAN)\n",
    "    if it % 100 == 0:\n",
    "        print('Iteration: {} ---> Discriminator loss: {:.4} | Generator loss: {:.4}'.format(it, D_loss_curr, G_loss_curr))\n",
    "        \n",
    "    \n",
    "    # Every 1000 Iteration Plot and save an example of the outputed sample images to see the progress\n",
    "    if it % 1000 == 0:\n",
    "            samples = sess.run(G_sample, feed_dict={z: sample_z(16, z_dim)})\n",
    "            fig = plot(samples)\n",
    "            plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "            i += 1\n",
    "            plt.close(fig)\n",
    "            \n",
    "# show the G&D loss plot at the end of training\n",
    "plt_dynamic(xs, ys, y2s, ax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the results of the 10.000 iteration, we can clearly see somehow meaingfull digit like formations in the last stag. of course the more you iterate the more you will get better results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](img/10.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot is for loss during a 5000 Iteration (Generator) is the Blue line and (Discriminator) in Green, we can see that it is not quite converging and not perfectly stable, However still doing much better work than the normal GAN as we saw in a previous graph I showed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/11.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see below the results is not quite correct as I said before that is because training GANs requires much more time in order to give good results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/12.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Therefor, I decided to hit a higher number of Iterations and see the results, The maximum number of Iterations I could reach is 10.000 iteration. after about more than 5 hours with my laptop suffering and freezing many times. \n",
    "\n",
    "##### The results of the 10.000 iteration is shown in the output of the training cell above, just before the 5000 iteration results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, Generative Adverserial Nets are a type of generative models, which they may come in a different types (DCGAN, WGAN, ..). And here in my project I showed an implementation of a WGAN and marked its propreties and how it is doing better that the normal GAN.\n",
    "\n",
    "I tried to explain everything I learned in the most simplest form, so I didn't write a lot of details (my intention was to make it understandable and not looking so advanced topic).\n",
    "\n",
    "One thing I would like to say about the dataset, I mentioned in my progress report that I am having a problem with the art dataset but at the end I found a collection of art training data, however it was very very big (35 GB) you may check it in the link below, also I realized that training on such dataset (art) would take MUCHE MUCHE MORE time to converge and to give acceptable results (since it is a more complicated data in its content). So that is why I finally decided to stick with the MNIST dataset (though it is a simpler one it took more than 5 hrs to give some results)\n",
    "\n",
    "https://drive.google.com/file/d/1yHqS2zXgCiI9LO4gN-X5W18QYXC5bbQS/view\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Side note; I know that I am doing a topic which is out of the scope as you told me, I tried somehow to include sklearn or something that we took in the class but I couldn't, since my hope was that we will take enough about deep learning part at the class (then I will no longer be out of the scope) but unfourtunatly we didn't have time to take enough from it. However, I hope that you take my efforts into considaration as I really learned a lot about something new and very important in the machine learning field.   ***\n",
    "\n",
    "## - The future of AI is Generative ! -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
